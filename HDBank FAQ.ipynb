{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up and return a configured Chrome WebDriver.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    # chrome_options.add_argument('--proxy-server=http://157.230.149.107:1040')  # Public proxy\n",
    "\n",
    "\n",
    "    # Initialize the Chrome driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "output_file = \"HDBank-FAQ.csv\"\n",
    "csv_lock = threading.Lock()  # Lock for thread-safe writing\n",
    "\n",
    "def write_headers():\n",
    "    if not os.path.exists(output_file):  # Check if file exists\n",
    "        with open(output_file, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                \"Questions\", \"Answer\"\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "\n",
    "# Call write_headers once to ensure headers are written if the file doesn't exist\n",
    "write_headers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(row, index):\n",
    "    logger.info(f\"Start Extraction of Faqs {index} form Web\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        with csv_lock:\n",
    "            with open(output_file, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "                writer.writerow(row)\n",
    "                \n",
    "        logger.info(f\"Extraction Complete of Faqs {index} form Web\")\n",
    "        return row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{index + 1}] Failed to insert info: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(driver):\n",
    "    \"\"\"Scrape one page.\"\"\"\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            section = driver.find_element(By.XPATH, \"//ul[contains(@class, 'list-questions')]\")\n",
    "        except NoSuchElementException:\n",
    "            logger.error(\"Failed to find the section with class 'UL list-questions'\", exc_info=True)\n",
    "\n",
    "        if section:\n",
    "            try:\n",
    "                elements = section.find_elements(By.XPATH, \".//li[contains(@class, 'item-question')]\")\n",
    "            except NoSuchElementException:\n",
    "                logger.error(\"Failed to find column divs inside row\", exc_info=True)\n",
    "\n",
    "        element_count = len(elements)\n",
    "        logger.info(f\"Found {element_count} elements to scrape on page\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(4, element_count)) as executor:\n",
    "            futures_to_indices = {}\n",
    "            for index, element in enumerate(elements):\n",
    "                try:\n",
    "                    # Question\n",
    "                    try:\n",
    "                        question = element.find_element(By.XPATH, \".//span[contains(@class, 'title-question')]\").text\n",
    "                    except NoSuchElementException:\n",
    "                        question = \"Not Found\"\n",
    "                    \n",
    "                    # Answer\n",
    "                    try:\n",
    "                        html = element.find_element(By.XPATH, \".//div[contains(@class, 'content-answer-box')]\").get_attribute(\"innerHTML\")\n",
    "                        soup = BeautifulSoup(html, \"html.parser\")\n",
    "                        answer = soup.get_text(separator=\"\\n\", strip=True)\n",
    "                    except NoSuchElementException:\n",
    "                        answer = 'Not Found'\n",
    "\n",
    "            \n",
    "                    row = {\n",
    "                        \"Questions\": question,\n",
    "                        \"Answer\": answer\n",
    "                    }\n",
    "                    \n",
    "                    future = executor.submit(store_data, row, index)\n",
    "                    futures_to_indices[future] = index\n",
    "                except Exception as e:\n",
    "                    print(f\"[{index + 1}] Failed to extract info: {e}\")\n",
    "            for future in as_completed(futures_to_indices):\n",
    "                index = futures_to_indices[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    logger.info(f\"Successfully scraped store at index {index}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error scraping store at index {index}: {e}\", exc_info=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during scraping: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_next_page(driver):\n",
    "    \"\"\"Navigate to the next page by clicking the 'Next' button.\"\"\"\n",
    "    try:\n",
    "        # Wait for the 'Next' button to be clickable\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//li[@class='next-btn']/a\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # Adjust this time if necessary\n",
    "        logger.info(\"Navigated to next page\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error navigating to next page: {e}\")\n",
    "        return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_pagination(start_url):\n",
    "    driver = setup_driver()\n",
    "    driver.get(start_url)\n",
    "    time.sleep(5)  # Initial load wait\n",
    "    count = 1\n",
    "    while True:\n",
    "        scrape_page(driver)  # Still pass current context\n",
    "\n",
    "        # Try to find and click the \"Next\" button\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.XPATH, \"//li[@class='next-btn']/a\")\n",
    "            classes = next_btn.get_attribute(\"class\")\n",
    "            \n",
    "            if \"disabled\" in classes:\n",
    "                logger.info(\"Reached last page. 'Next' button is disabled.\")\n",
    "                break  # Stop loop when the button is disabled\n",
    "            \n",
    "            # Scroll the element into view\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", next_btn)\n",
    "            time.sleep(2)  # Wait a bit for the scroll to finish and render\n",
    "\n",
    "            # Optional: highlight the element before screenshot (for better visibility)\n",
    "            driver.execute_script(\"arguments[0].style.border='3px solid red'\", next_btn)\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"//li[@class='next-btn']/a\"))\n",
    "                    )\n",
    "            # driver.save_screenshot(f\"page{count}.png\")\n",
    "            # Proceed to click\n",
    "            next_button.click()\n",
    "            time.sleep(10)\n",
    "            logger.info(f\"'Next' button found. And Goes TO next Page({count}).\")\n",
    "            count= count + 1\n",
    "        except NoSuchElementException:\n",
    "            logger.error(\"No 'Next' button found. Stopping pagination.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error during pagination: {e}\", exc_info=True)\n",
    "            break\n",
    "\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 14:28:11,705 - INFO - Found 4 elements to scrape on page\n",
      "2025-04-24 14:28:11,773 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:28:11,779 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:28:11,825 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:28:11,827 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:28:11,879 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:28:11,881 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:28:11,926 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:28:11,926 - INFO - Start Extraction of Faqs 3 form Web\n",
      "2025-04-24 14:28:11,927 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:28:11,929 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:28:11,931 - INFO - Extraction Complete of Faqs 3 form Web\n",
      "2025-04-24 14:28:11,932 - INFO - Successfully scraped store at index 3\n",
      "2025-04-24 14:28:24,182 - INFO - 'Next' button found. And Goes TO next Page(1).\n",
      "2025-04-24 14:28:24,216 - INFO - Found 4 elements to scrape on page\n",
      "2025-04-24 14:28:24,265 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:28:24,269 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:28:24,320 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:28:24,322 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:28:24,369 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:28:24,370 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:28:24,424 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:28:24,424 - INFO - Start Extraction of Faqs 3 form Web\n",
      "2025-04-24 14:28:24,425 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:28:24,428 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:28:24,428 - INFO - Extraction Complete of Faqs 3 form Web\n",
      "2025-04-24 14:28:24,431 - INFO - Successfully scraped store at index 3\n",
      "2025-04-24 14:28:36,625 - INFO - 'Next' button found. And Goes TO next Page(2).\n",
      "2025-04-24 14:28:36,649 - INFO - Found 4 elements to scrape on page\n",
      "2025-04-24 14:28:36,697 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:28:36,701 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:28:36,747 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:28:36,749 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:28:36,796 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:28:36,798 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:28:36,842 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:28:36,842 - INFO - Start Extraction of Faqs 3 form Web\n",
      "2025-04-24 14:28:36,843 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:28:36,845 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:28:36,845 - INFO - Extraction Complete of Faqs 3 form Web\n",
      "2025-04-24 14:28:36,847 - INFO - Successfully scraped store at index 3\n",
      "2025-04-24 14:28:49,020 - INFO - 'Next' button found. And Goes TO next Page(3).\n",
      "2025-04-24 14:28:49,045 - INFO - Found 4 elements to scrape on page\n",
      "2025-04-24 14:28:49,089 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:28:49,093 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:28:49,220 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:28:49,222 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:28:49,262 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:28:49,265 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:28:49,306 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:28:49,307 - INFO - Start Extraction of Faqs 3 form Web\n",
      "2025-04-24 14:28:49,308 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:28:49,309 - INFO - Extraction Complete of Faqs 3 form Web\n",
      "2025-04-24 14:28:49,310 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:28:49,312 - INFO - Successfully scraped store at index 3\n",
      "2025-04-24 14:29:01,479 - INFO - 'Next' button found. And Goes TO next Page(4).\n",
      "2025-04-24 14:29:01,506 - INFO - Found 4 elements to scrape on page\n",
      "2025-04-24 14:29:01,550 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:29:01,556 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:29:01,602 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:29:01,604 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:29:01,645 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:29:01,647 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:29:01,691 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:29:01,691 - INFO - Start Extraction of Faqs 3 form Web\n",
      "2025-04-24 14:29:01,692 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:29:01,694 - INFO - Extraction Complete of Faqs 3 form Web\n",
      "2025-04-24 14:29:01,694 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:29:01,696 - INFO - Successfully scraped store at index 3\n",
      "2025-04-24 14:29:13,903 - INFO - 'Next' button found. And Goes TO next Page(5).\n",
      "2025-04-24 14:29:13,953 - INFO - Found 4 elements to scrape on page\n",
      "2025-04-24 14:29:13,998 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:29:14,005 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:29:14,057 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:29:14,059 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:29:14,114 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:29:14,117 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:29:14,167 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:29:14,167 - INFO - Start Extraction of Faqs 3 form Web\n",
      "2025-04-24 14:29:14,168 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:29:14,170 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:29:14,171 - INFO - Extraction Complete of Faqs 3 form Web\n",
      "2025-04-24 14:29:14,172 - INFO - Successfully scraped store at index 3\n",
      "2025-04-24 14:29:26,334 - INFO - 'Next' button found. And Goes TO next Page(6).\n",
      "2025-04-24 14:29:26,365 - INFO - Found 3 elements to scrape on page\n",
      "2025-04-24 14:29:26,417 - INFO - Start Extraction of Faqs 0 form Web\n",
      "2025-04-24 14:29:26,421 - INFO - Extraction Complete of Faqs 0 form Web\n",
      "2025-04-24 14:29:26,463 - INFO - Start Extraction of Faqs 1 form Web\n",
      "2025-04-24 14:29:26,466 - INFO - Extraction Complete of Faqs 1 form Web\n",
      "2025-04-24 14:29:26,509 - INFO - Successfully scraped store at index 1\n",
      "2025-04-24 14:29:26,509 - INFO - Start Extraction of Faqs 2 form Web\n",
      "2025-04-24 14:29:26,510 - INFO - Successfully scraped store at index 0\n",
      "2025-04-24 14:29:26,513 - INFO - Extraction Complete of Faqs 2 form Web\n",
      "2025-04-24 14:29:26,515 - INFO - Successfully scraped store at index 2\n",
      "2025-04-24 14:29:26,529 - ERROR - No 'Next' button found. Stopping pagination.\n"
     ]
    }
   ],
   "source": [
    "# Start the scraping process\n",
    "# scrape_with_pagination(\"https://hdbank.com.vn/en/personal/QnA\")\n",
    "scrape_with_pagination(\"https://hdbank.com.vn/en/corporate/QnA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
