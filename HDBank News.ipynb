{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import logging\n",
    "import urllib.parse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Set up and return a configured Chrome WebDriver.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    # chrome_options.add_argument('--proxy-server=http://157.230.149.107:1040')  # Public proxy\n",
    "\n",
    "\n",
    "    # Initialize the Chrome driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "output_file = \"HDBank-news.csv\"\n",
    "csv_lock = threading.Lock()  # Lock for thread-safe writing\n",
    "\n",
    "def write_headers():\n",
    "    if not os.path.exists(output_file):  # Check if file exists\n",
    "        with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\n",
    "                \"image\", \"title\", \"description\", \"date\",\"detail\", \"detail_images\"\n",
    "            ])\n",
    "            writer.writeheader()\n",
    "\n",
    "# Call write_headers once to ensure headers are written if the file doesn't exist\n",
    "write_headers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news(url,index):\n",
    "    logger.info(f\"Start Extraction of News detail {index} form Web\")\n",
    "    item = setup_driver()\n",
    "    item.get(url)\n",
    "    time.sleep(15)\n",
    "    try:\n",
    "        # Assuming `driver` is already loaded on the detail page\n",
    "        wrapper = item.find_element(By.CLASS_NAME, \"wrapper-content\")\n",
    "\n",
    "        item.save_screenshot(\"page.png\")\n",
    "        \n",
    "        # Get all <p> tags inside the wrapper\n",
    "        paragraphs = wrapper.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "        all_paragraphs = []\n",
    "        all_images = []\n",
    "        for p in paragraphs:\n",
    "            # Collect text if exists\n",
    "            text = p.text.strip()\n",
    "            if text:\n",
    "                all_paragraphs.append(text)\n",
    "\n",
    "            # Check for <img> tag inside this <p>\n",
    "            try:\n",
    "                img = p.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "                all_images.append(img)\n",
    "            except:\n",
    "                pass  # Skip if no image\n",
    "            \n",
    "        return all_images, all_paragraphs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{index + 1}] Failed to extract info: {e}\")\n",
    "        return [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(row, index,href):\n",
    "    logger.info(f\"Start Extraction of News {index} form Web\")\n",
    "    try:\n",
    "        all_images, all_paragraphs = extract_news(href,index)\n",
    "        row['detail']=all_paragraphs\n",
    "        row['detail_images']=all_images\n",
    "    except Exception as e:\n",
    "        print(f\"[{index + 1}] Failed to Extract Detail: {e}\")\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        with csv_lock:\n",
    "            with open(output_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "                writer.writerow(row)\n",
    "                \n",
    "        logger.info(f\"Extraction Complete of News {index} form Web\")\n",
    "        return row\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{index + 1}] Failed to insert info: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 10:47:14,796 - INFO - Found 864 elements to scrape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 elements.\n"
     ]
    }
   ],
   "source": [
    "driver = setup_driver()\n",
    "driver.get(\"https://hdbank.com.vn/news/moi-nhat?page=1000\")\n",
    "\n",
    "time.sleep(15)\n",
    "try:\n",
    "    elements = driver.find_elements(By.XPATH, \"//li[contains(@class, 'item-news')]\")\n",
    "    element_count = len(elements)\n",
    "    logger.info(f\"Found {element_count} elements to scrape\")\n",
    "    driver.save_screenshot(\"page.png\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during scraping: {str(e)}\", exc_info=True)\n",
    "element_count = len(elements)\n",
    "print(f\"Found {element_count} elements.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 10:47:29,763 - INFO - Start Extraction of News 559 form Web\n",
      "2025-04-15 10:47:29,767 - INFO - Start Extraction of News detail 559 form Web\n",
      "2025-04-15 10:47:29,919 - INFO - Start Extraction of News 560 form Web\n",
      "2025-04-15 10:47:29,924 - INFO - Start Extraction of News detail 560 form Web\n",
      "2025-04-15 10:47:30,112 - INFO - Start Extraction of News 561 form Web\n",
      "2025-04-15 10:47:30,116 - INFO - Start Extraction of News detail 561 form Web\n",
      "2025-04-15 10:47:30,329 - INFO - Start Extraction of News 562 form Web\n",
      "2025-04-15 10:47:30,341 - INFO - Start Extraction of News detail 562 form Web\n",
      "2025-04-15 10:48:06,597 - INFO - Extraction Complete of News 562 form Web\n",
      "2025-04-15 10:48:06,601 - INFO - Start Extraction of News 563 form Web\n",
      "2025-04-15 10:48:06,605 - INFO - Start Extraction of News detail 563 form Web\n",
      "2025-04-15 10:48:13,709 - INFO - Extraction Complete of News 560 form Web\n",
      "2025-04-15 10:48:13,712 - INFO - Start Extraction of News 564 form Web\n",
      "2025-04-15 10:48:13,716 - INFO - Start Extraction of News detail 564 form Web\n",
      "2025-04-15 10:48:19,052 - INFO - Extraction Complete of News 561 form Web\n",
      "2025-04-15 10:48:19,055 - INFO - Start Extraction of News 565 form Web\n",
      "2025-04-15 10:48:19,057 - INFO - Start Extraction of News detail 565 form Web\n",
      "2025-04-15 10:48:32,275 - INFO - Extraction Complete of News 559 form Web\n",
      "2025-04-15 10:48:32,277 - INFO - Start Extraction of News 566 form Web\n",
      "2025-04-15 10:48:32,280 - INFO - Start Extraction of News detail 566 form Web\n",
      "2025-04-15 10:48:45,034 - INFO - Extraction Complete of News 563 form Web\n",
      "2025-04-15 10:48:45,038 - INFO - Start Extraction of News 567 form Web\n",
      "2025-04-15 10:48:45,039 - INFO - Start Extraction of News detail 567 form Web\n",
      "2025-04-15 10:48:53,030 - INFO - Extraction Complete of News 564 form Web\n",
      "2025-04-15 10:48:53,032 - INFO - Start Extraction of News 568 form Web\n",
      "2025-04-15 10:48:53,036 - INFO - Start Extraction of News detail 568 form Web\n",
      "2025-04-15 10:49:24,476 - INFO - Extraction Complete of News 566 form Web\n",
      "2025-04-15 10:49:24,479 - INFO - Start Extraction of News 569 form Web\n",
      "2025-04-15 10:49:24,481 - INFO - Start Extraction of News detail 569 form Web\n",
      "2025-04-15 10:49:28,268 - INFO - Extraction Complete of News 567 form Web\n",
      "2025-04-15 10:49:28,271 - INFO - Start Extraction of News 570 form Web\n",
      "2025-04-15 10:49:28,273 - INFO - Start Extraction of News detail 570 form Web\n",
      "2025-04-15 10:49:30,214 - INFO - Extraction Complete of News 565 form Web\n",
      "2025-04-15 10:49:30,217 - INFO - Start Extraction of News 571 form Web\n",
      "2025-04-15 10:49:30,220 - INFO - Start Extraction of News detail 571 form Web\n",
      "2025-04-15 10:49:48,997 - INFO - Extraction Complete of News 568 form Web\n",
      "2025-04-15 10:49:49,000 - INFO - Start Extraction of News 572 form Web\n",
      "2025-04-15 10:49:49,003 - INFO - Start Extraction of News detail 572 form Web\n",
      "2025-04-15 10:50:26,806 - INFO - Extraction Complete of News 571 form Web\n",
      "2025-04-15 10:50:26,809 - INFO - Start Extraction of News 573 form Web\n",
      "2025-04-15 10:50:26,811 - INFO - Start Extraction of News detail 573 form Web\n",
      "2025-04-15 10:50:28,998 - INFO - Extraction Complete of News 569 form Web\n",
      "2025-04-15 10:50:29,001 - INFO - Start Extraction of News 574 form Web\n",
      "2025-04-15 10:50:29,004 - INFO - Start Extraction of News detail 574 form Web\n",
      "2025-04-15 10:50:36,539 - INFO - Extraction Complete of News 572 form Web\n",
      "2025-04-15 10:50:36,543 - INFO - Start Extraction of News 575 form Web\n",
      "2025-04-15 10:50:36,546 - INFO - Start Extraction of News detail 575 form Web\n",
      "2025-04-15 10:50:38,384 - INFO - Extraction Complete of News 570 form Web\n",
      "2025-04-15 10:50:38,387 - INFO - Start Extraction of News 576 form Web\n",
      "2025-04-15 10:50:38,389 - INFO - Start Extraction of News detail 576 form Web\n",
      "2025-04-15 10:51:29,722 - INFO - Extraction Complete of News 576 form Web\n",
      "2025-04-15 10:51:29,726 - INFO - Start Extraction of News 577 form Web\n",
      "2025-04-15 10:51:29,728 - INFO - Start Extraction of News detail 577 form Web\n",
      "2025-04-15 10:51:31,629 - INFO - Extraction Complete of News 573 form Web\n",
      "2025-04-15 10:51:31,632 - INFO - Start Extraction of News 578 form Web\n",
      "2025-04-15 10:51:31,634 - INFO - Start Extraction of News detail 578 form Web\n",
      "2025-04-15 10:51:33,930 - INFO - Extraction Complete of News 574 form Web\n",
      "2025-04-15 10:51:33,932 - INFO - Start Extraction of News 579 form Web\n",
      "2025-04-15 10:51:33,935 - INFO - Start Extraction of News detail 579 form Web\n",
      "2025-04-15 10:51:44,130 - INFO - Extraction Complete of News 575 form Web\n",
      "2025-04-15 10:51:44,133 - INFO - Start Extraction of News 580 form Web\n",
      "2025-04-15 10:51:44,136 - INFO - Start Extraction of News detail 580 form Web\n",
      "2025-04-15 10:52:06,567 - INFO - Successfully scraped store at index 566\n",
      "2025-04-15 10:52:06,570 - INFO - Successfully scraped store at index 568\n",
      "2025-04-15 10:52:06,572 - INFO - Successfully scraped store at index 573\n",
      "2025-04-15 10:52:06,574 - INFO - Successfully scraped store at index 572\n",
      "2025-04-15 10:52:06,576 - INFO - Successfully scraped store at index 575\n",
      "2025-04-15 10:52:06,578 - INFO - Successfully scraped store at index 571\n",
      "2025-04-15 10:52:06,580 - INFO - Successfully scraped store at index 567\n",
      "2025-04-15 10:52:06,582 - INFO - Successfully scraped store at index 565\n",
      "2025-04-15 10:52:06,584 - INFO - Successfully scraped store at index 560\n",
      "2025-04-15 10:52:06,586 - INFO - Successfully scraped store at index 562\n",
      "2025-04-15 10:52:06,588 - INFO - Successfully scraped store at index 576\n",
      "2025-04-15 10:52:06,591 - INFO - Successfully scraped store at index 561\n",
      "2025-04-15 10:52:06,593 - INFO - Successfully scraped store at index 570\n",
      "2025-04-15 10:52:06,595 - INFO - Successfully scraped store at index 569\n",
      "2025-04-15 10:52:06,597 - INFO - Successfully scraped store at index 564\n",
      "2025-04-15 10:52:06,599 - INFO - Successfully scraped store at index 574\n",
      "2025-04-15 10:52:06,601 - INFO - Successfully scraped store at index 563\n",
      "2025-04-15 10:52:06,603 - INFO - Successfully scraped store at index 559\n",
      "2025-04-15 10:52:23,018 - INFO - Extraction Complete of News 579 form Web\n",
      "2025-04-15 10:52:23,020 - INFO - Start Extraction of News 581 form Web\n",
      "2025-04-15 10:52:23,021 - INFO - Successfully scraped store at index 579\n",
      "2025-04-15 10:52:23,022 - INFO - Start Extraction of News detail 581 form Web\n",
      "2025-04-15 10:52:28,815 - INFO - Extraction Complete of News 577 form Web\n",
      "2025-04-15 10:52:28,818 - INFO - Start Extraction of News 582 form Web\n",
      "2025-04-15 10:52:28,820 - INFO - Start Extraction of News detail 582 form Web\n",
      "2025-04-15 10:52:28,822 - INFO - Successfully scraped store at index 577\n",
      "2025-04-15 10:52:35,070 - INFO - Extraction Complete of News 580 form Web\n",
      "2025-04-15 10:52:35,074 - INFO - Start Extraction of News 583 form Web\n",
      "2025-04-15 10:52:35,075 - INFO - Successfully scraped store at index 580\n",
      "2025-04-15 10:52:35,076 - INFO - Start Extraction of News detail 583 form Web\n",
      "2025-04-15 10:52:36,729 - INFO - Extraction Complete of News 578 form Web\n",
      "2025-04-15 10:52:36,731 - INFO - Start Extraction of News 584 form Web\n",
      "2025-04-15 10:52:36,734 - INFO - Start Extraction of News detail 584 form Web\n",
      "2025-04-15 10:52:36,732 - INFO - Successfully scraped store at index 578\n",
      "2025-04-15 10:53:13,035 - INFO - Extraction Complete of News 581 form Web\n",
      "2025-04-15 10:53:13,038 - INFO - Start Extraction of News 585 form Web\n",
      "2025-04-15 10:53:13,039 - INFO - Successfully scraped store at index 581\n",
      "2025-04-15 10:53:13,040 - INFO - Start Extraction of News detail 585 form Web\n",
      "2025-04-15 10:53:25,076 - INFO - Extraction Complete of News 582 form Web\n",
      "2025-04-15 10:53:25,080 - INFO - Start Extraction of News 586 form Web\n",
      "2025-04-15 10:53:25,088 - INFO - Start Extraction of News detail 586 form Web\n",
      "2025-04-15 10:53:25,088 - INFO - Successfully scraped store at index 582\n",
      "2025-04-15 10:54:05,055 - INFO - Extraction Complete of News 583 form Web\n",
      "2025-04-15 10:54:05,058 - INFO - Start Extraction of News 587 form Web\n",
      "2025-04-15 10:54:05,060 - INFO - Successfully scraped store at index 583\n",
      "2025-04-15 10:54:05,060 - INFO - Start Extraction of News detail 587 form Web\n",
      "2025-04-15 10:54:20,704 - INFO - Extraction Complete of News 585 form Web\n",
      "2025-04-15 10:54:20,707 - INFO - Start Extraction of News 588 form Web\n",
      "2025-04-15 10:54:20,710 - INFO - Start Extraction of News detail 588 form Web\n",
      "2025-04-15 10:54:20,724 - INFO - Successfully scraped store at index 585\n",
      "2025-04-15 10:54:34,034 - INFO - Extraction Complete of News 586 form Web\n",
      "2025-04-15 10:54:34,038 - INFO - Start Extraction of News 589 form Web\n",
      "2025-04-15 10:54:34,042 - INFO - Start Extraction of News detail 589 form Web\n",
      "2025-04-15 10:54:34,043 - INFO - Successfully scraped store at index 586\n",
      "2025-04-15 10:54:59,405 - INFO - Extraction Complete of News 587 form Web\n",
      "2025-04-15 10:54:59,408 - INFO - Start Extraction of News 590 form Web\n",
      "2025-04-15 10:54:59,410 - INFO - Successfully scraped store at index 587\n",
      "2025-04-15 10:54:59,411 - INFO - Start Extraction of News detail 590 form Web\n",
      "2025-04-15 10:55:10,052 - INFO - Extraction Complete of News 584 form Web\n",
      "2025-04-15 10:55:10,055 - INFO - Start Extraction of News 591 form Web\n",
      "2025-04-15 10:55:10,056 - INFO - Successfully scraped store at index 584\n",
      "2025-04-15 10:55:10,058 - INFO - Start Extraction of News detail 591 form Web\n",
      "2025-04-15 10:55:15,244 - INFO - Extraction Complete of News 588 form Web\n",
      "2025-04-15 10:55:15,248 - INFO - Start Extraction of News 592 form Web\n",
      "2025-04-15 10:55:15,249 - INFO - Successfully scraped store at index 588\n",
      "2025-04-15 10:55:15,252 - INFO - Start Extraction of News detail 592 form Web\n",
      "2025-04-15 10:55:31,315 - INFO - Extraction Complete of News 589 form Web\n",
      "2025-04-15 10:55:31,318 - INFO - Start Extraction of News 593 form Web\n",
      "2025-04-15 10:55:31,343 - INFO - Start Extraction of News detail 593 form Web\n",
      "2025-04-15 10:55:31,318 - INFO - Successfully scraped store at index 589\n",
      "2025-04-15 10:55:55,858 - INFO - Extraction Complete of News 590 form Web\n",
      "2025-04-15 10:55:55,860 - INFO - Start Extraction of News 594 form Web\n",
      "2025-04-15 10:55:55,862 - INFO - Start Extraction of News detail 594 form Web\n",
      "2025-04-15 10:55:55,863 - INFO - Successfully scraped store at index 590\n",
      "2025-04-15 10:56:07,441 - INFO - Extraction Complete of News 591 form Web\n",
      "2025-04-15 10:56:07,444 - INFO - Start Extraction of News 595 form Web\n",
      "2025-04-15 10:56:07,445 - INFO - Successfully scraped store at index 591\n",
      "2025-04-15 10:56:07,447 - INFO - Start Extraction of News detail 595 form Web\n",
      "2025-04-15 10:56:31,138 - INFO - Extraction Complete of News 592 form Web\n",
      "2025-04-15 10:56:31,141 - INFO - Start Extraction of News 596 form Web\n",
      "2025-04-15 10:56:31,141 - INFO - Successfully scraped store at index 592\n",
      "2025-04-15 10:56:31,144 - INFO - Start Extraction of News detail 596 form Web\n",
      "2025-04-15 10:56:36,781 - INFO - Extraction Complete of News 593 form Web\n",
      "2025-04-15 10:56:36,792 - INFO - Start Extraction of News 597 form Web\n",
      "2025-04-15 10:56:36,794 - INFO - Successfully scraped store at index 593\n",
      "2025-04-15 10:56:36,890 - INFO - Start Extraction of News detail 597 form Web\n",
      "2025-04-15 10:56:48,592 - INFO - Extraction Complete of News 594 form Web\n",
      "2025-04-15 10:56:48,596 - INFO - Start Extraction of News 598 form Web\n",
      "2025-04-15 10:56:48,596 - INFO - Successfully scraped store at index 594\n",
      "2025-04-15 10:56:48,599 - INFO - Start Extraction of News detail 598 form Web\n",
      "2025-04-15 10:57:00,950 - INFO - Extraction Complete of News 595 form Web\n",
      "2025-04-15 10:57:00,955 - INFO - Start Extraction of News 599 form Web\n",
      "2025-04-15 10:57:00,956 - INFO - Successfully scraped store at index 595\n",
      "2025-04-15 10:57:00,958 - INFO - Start Extraction of News detail 599 form Web\n",
      "2025-04-15 10:57:02,122 - INFO - Extraction Complete of News 597 form Web\n",
      "2025-04-15 10:57:02,126 - INFO - Start Extraction of News 600 form Web\n",
      "2025-04-15 10:57:02,127 - INFO - Successfully scraped store at index 597\n",
      "2025-04-15 10:57:02,138 - INFO - Start Extraction of News detail 600 form Web\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[598] Failed to extract info: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".wrapper-content\"}\n",
      "  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7689A5335+78597]\n",
      "\tGetHandleVerifier [0x00007FF7689A5390+78688]\n",
      "\t(No symbol) [0x00007FF7687591AA]\n",
      "\t(No symbol) [0x00007FF7687AF149]\n",
      "\t(No symbol) [0x00007FF7687AF3FC]\n",
      "\t(No symbol) [0x00007FF768802467]\n",
      "\t(No symbol) [0x00007FF7687D712F]\n",
      "\t(No symbol) [0x00007FF7687FF2BB]\n",
      "\t(No symbol) [0x00007FF7687D6EC3]\n",
      "\t(No symbol) [0x00007FF7687A03F8]\n",
      "\t(No symbol) [0x00007FF7687A1163]\n",
      "\tGetHandleVerifier [0x00007FF768C4EEED+2870973]\n",
      "\tGetHandleVerifier [0x00007FF768C49698+2848360]\n",
      "\tGetHandleVerifier [0x00007FF768C66973+2967875]\n",
      "\tGetHandleVerifier [0x00007FF7689C017A+188746]\n",
      "\tGetHandleVerifier [0x00007FF7689C845F+222255]\n",
      "\tGetHandleVerifier [0x00007FF7689AD2B4+111236]\n",
      "\tGetHandleVerifier [0x00007FF7689AD462+111666]\n",
      "\tGetHandleVerifier [0x00007FF768993589+5465]\n",
      "\tBaseThreadInitThunk [0x00007FF9BA6EE8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FF9BC51BF6C+44]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 10:57:29,776 - INFO - Extraction Complete of News 596 form Web\n",
      "2025-04-15 10:57:29,785 - INFO - Start Extraction of News 601 form Web\n",
      "2025-04-15 10:57:29,786 - INFO - Successfully scraped store at index 596\n",
      "2025-04-15 10:57:29,791 - INFO - Start Extraction of News detail 601 form Web\n",
      "2025-04-15 10:57:49,908 - INFO - Extraction Complete of News 598 form Web\n",
      "2025-04-15 10:57:49,912 - INFO - Start Extraction of News 602 form Web\n",
      "2025-04-15 10:57:49,912 - INFO - Successfully scraped store at index 598\n",
      "2025-04-15 10:57:49,914 - INFO - Start Extraction of News detail 602 form Web\n",
      "2025-04-15 10:57:53,930 - INFO - Extraction Complete of News 599 form Web\n",
      "2025-04-15 10:57:53,934 - INFO - Start Extraction of News 603 form Web\n",
      "2025-04-15 10:57:53,939 - INFO - Start Extraction of News detail 603 form Web\n",
      "2025-04-15 10:57:53,935 - INFO - Successfully scraped store at index 599\n",
      "2025-04-15 10:58:16,589 - INFO - Extraction Complete of News 600 form Web\n",
      "2025-04-15 10:58:16,593 - INFO - Start Extraction of News 604 form Web\n",
      "2025-04-15 10:58:16,605 - INFO - Start Extraction of News detail 604 form Web\n",
      "2025-04-15 10:58:16,602 - INFO - Successfully scraped store at index 600\n",
      "2025-04-15 10:58:47,214 - INFO - Extraction Complete of News 601 form Web\n",
      "2025-04-15 10:58:47,217 - INFO - Start Extraction of News 605 form Web\n",
      "2025-04-15 10:58:47,220 - INFO - Start Extraction of News detail 605 form Web\n",
      "2025-04-15 10:58:47,228 - INFO - Successfully scraped store at index 601\n",
      "2025-04-15 10:58:55,788 - INFO - Extraction Complete of News 602 form Web\n",
      "2025-04-15 10:58:55,791 - INFO - Start Extraction of News 606 form Web\n",
      "2025-04-15 10:58:55,792 - INFO - Successfully scraped store at index 602\n",
      "2025-04-15 10:58:55,793 - INFO - Start Extraction of News detail 606 form Web\n",
      "2025-04-15 10:59:50,173 - INFO - Extraction Complete of News 603 form Web\n",
      "2025-04-15 10:59:50,176 - INFO - Start Extraction of News 607 form Web\n",
      "2025-04-15 10:59:50,177 - INFO - Successfully scraped store at index 603\n",
      "2025-04-15 10:59:50,179 - INFO - Start Extraction of News detail 607 form Web\n",
      "2025-04-15 11:00:04,208 - INFO - Extraction Complete of News 604 form Web\n",
      "2025-04-15 11:00:04,212 - INFO - Start Extraction of News 608 form Web\n",
      "2025-04-15 11:00:04,213 - INFO - Successfully scraped store at index 604\n",
      "2025-04-15 11:00:04,215 - INFO - Start Extraction of News detail 608 form Web\n",
      "2025-04-15 11:00:44,849 - INFO - Extraction Complete of News 605 form Web\n",
      "2025-04-15 11:00:44,853 - INFO - Start Extraction of News 609 form Web\n",
      "2025-04-15 11:00:44,854 - INFO - Successfully scraped store at index 605\n",
      "2025-04-15 11:00:44,856 - INFO - Start Extraction of News detail 609 form Web\n",
      "2025-04-15 11:01:10,215 - INFO - Extraction Complete of News 606 form Web\n",
      "2025-04-15 11:01:10,218 - INFO - Start Extraction of News 610 form Web\n",
      "2025-04-15 11:01:10,219 - INFO - Successfully scraped store at index 606\n",
      "2025-04-15 11:01:10,220 - INFO - Start Extraction of News detail 610 form Web\n",
      "2025-04-15 11:01:21,464 - INFO - Extraction Complete of News 607 form Web\n",
      "2025-04-15 11:01:21,468 - INFO - Start Extraction of News 611 form Web\n",
      "2025-04-15 11:01:21,468 - INFO - Successfully scraped store at index 607\n",
      "2025-04-15 11:01:21,471 - INFO - Start Extraction of News detail 611 form Web\n",
      "2025-04-15 11:01:23,343 - INFO - Extraction Complete of News 608 form Web\n",
      "2025-04-15 11:01:23,350 - INFO - Start Extraction of News 612 form Web\n",
      "2025-04-15 11:01:23,353 - INFO - Start Extraction of News detail 612 form Web\n",
      "2025-04-15 11:01:23,350 - INFO - Successfully scraped store at index 608\n"
     ]
    }
   ],
   "source": [
    "element_count = len(elements)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=min(4, element_count)) as executor:\n",
    "    futures_to_indices = {}\n",
    "    \n",
    "    for index, element in enumerate(elements):\n",
    "        try:\n",
    "            if index > 558:\n",
    "                # Image\n",
    "                try:\n",
    "                    img = element.find_element(By.XPATH, \".//div[contains(@class, 'news-left-box')]//img\").get_attribute(\"src\")\n",
    "                except:\n",
    "                    img = \"\"\n",
    "\n",
    "                # Title\n",
    "                title = element.find_element(By.XPATH, \".//div[contains(@class, 'news-right-box')]//p[contains(@class, 'news-title')]\").text.strip()\n",
    "\n",
    "                # Description\n",
    "                try:\n",
    "                    description = element.find_element(By.XPATH, \".//p[contains(@class, 'news-description')]\").text.strip()\n",
    "                except:\n",
    "                    description = \"\"\n",
    "\n",
    "                # Date\n",
    "                date = element.find_element(By.XPATH, \".//p[contains(@class, 'news-date')]/time\").text.strip()\n",
    "\n",
    "                row = {\n",
    "                    \"image\": img,\n",
    "                    \"title\": title,\n",
    "                    \"description\": description,\n",
    "                    \"date\": date\n",
    "                }\n",
    "                href = elements[1].find_element(By.XPATH, \".//div[contains(@class, 'news-left-box')]/a\").get_attribute(\"href\")\n",
    "                future = executor.submit(store_data, row, index, href)\n",
    "                futures_to_indices[future] = index\n",
    "        except Exception as e:\n",
    "            print(f\"[{index + 1}] Failed to extract info: {e}\")\n",
    "\n",
    "    for future in as_completed(futures_to_indices):\n",
    "        index = futures_to_indices[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            logger.info(f\"Successfully scraped store at index {index}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping store at index {index}: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
